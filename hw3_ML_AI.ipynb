{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0bv7sM9bej"
      },
      "source": [
        "# Part 1 (50 points)\n",
        "In this part you will implement a neural network from scratch. You cannot use any existing\n",
        "Deep Learning Framework. You can utilize NumPy and Pandas libraries to perform efficient\n",
        "calculations. Refer to Lecture 5 slides for details on computations required.\n",
        "Write a Class called NeuralNetwork that has at least the following methods (you are free to add\n",
        "your own methods too):\n",
        "a. Initialization method.\n",
        "b. Forward propagation method that performs forward propagation calculations.\n",
        "c. Backward propagation method that implements the backpropagation algorithm\n",
        "discussed in class.\n",
        "d. Train method that includes the code for gradient descent.\n",
        "e. Cost method that calculates the loss function.\n",
        "f. Predict method that calculates the predictions for the test set.\n",
        "Test your NeuralNetwork Class with the dataset you selected. If the dataset is big, you may\n",
        "notice inefficiencies in runtime. Try incorporating different versions of gradient descent to\n",
        "improve that (Minibatch, Stochastic etc.). You may choose to use only a subset of your data for\n",
        "this task (or any other technique). Explain which technique you followed and why.\n",
        "\n",
        "## Comments:\n",
        "\n",
        "For the below code, I mainly follow this link https://www.kaggle.com/code/ihalil95/building-two-layer-neural-networks-from-scratch and ChatGPT for guidance.\n",
        "\n",
        "For the dataset, I decide to use MNIST: http://yann.lecun.com/exdb/mnist/\n",
        "This dataset contains 60000 hand written digit examples in the training sets, and 10000 examples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jxMbc3C901PG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
        "      # initialize the class\n",
        "        self.params = self.initialize_parameters(input_size, hidden_layer_size, output_size)\n",
        "\n",
        "    def initialize_parameters(self, input_size, hidden_layer_size, output_size):\n",
        "      # Intialize parameters. Weights: normal distirbution. Bias: zeros.\n",
        "        np.random.seed(1)\n",
        "        W1 = np.random.randn(hidden_layer_size, input_size) * 0.01\n",
        "        b1 = np.zeros((hidden_layer_size, 1))\n",
        "        W2 = np.random.randn(output_size, hidden_layer_size) * 0.01\n",
        "        b2 = np.zeros((output_size, 1))\n",
        "        return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "    def relu(self, Z):\n",
        "      # Relu activation\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "      # Softmax for multiclass classification\n",
        "        e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "        return e_Z / np.sum(e_Z, axis=0, keepdims=True)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "      # Perform forward progragation for two layer network\n",
        "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
        "\n",
        "        Z1 = np.dot(W1, X) + b1\n",
        "        A1 = self.relu(Z1)\n",
        "        Z2 = np.dot(W2, A1) + b2\n",
        "        A2 = self.softmax(Z2)\n",
        "\n",
        "        self.cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
        "        return A2\n",
        "\n",
        "    def compute_cost(self, Y_hat, Y):\n",
        "      # lost function: cross entropy loss\n",
        "        m = Y.shape[1]\n",
        "        cost = -np.sum(np.log(Y_hat) * Y) / m\n",
        "        cost = np.squeeze(cost)\n",
        "        return cost\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "      # derivative of relu\n",
        "        return Z > 0\n",
        "\n",
        "    def backward_propagation(self, X, Y):\n",
        "      # perform backward progagation\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        Z1, A1, A2 = self.cache['Z1'], self.cache['A1'], self.cache['A2']\n",
        "        m = X.shape[1]\n",
        "\n",
        "        dZ2 = A2 - Y # A2: output of cross entropy loss. Y: one hot encoding format of true label.\n",
        "        # They share same dimensionality, so direct difference works.\n",
        "        dW2 = np.dot(dZ2, A1.T) / m\n",
        "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "        dZ1 = np.dot(W2.T, dZ2) * self.relu_derivative(Z1)\n",
        "        dW1 = np.dot(dZ1, X.T) / m\n",
        "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "        self.grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "\n",
        "    def update_parameters(self, learning_rate=0.01):\n",
        "      # update parameters with learning rate\n",
        "        for param in self.params:\n",
        "            self.params[param] -= learning_rate * self.grads[\"d\" + param]\n",
        "\n",
        "    def predict(self, X):\n",
        "      # prediction\n",
        "        A2 = self.forward_propagation(X)\n",
        "        return np.argmax(A2, axis=0)\n",
        "\n",
        "    def fit(self, X, Y, iterations=1000, learning_rate=0.01):\n",
        "      # traditional training using gradient descent\n",
        "        for i in range(iterations):\n",
        "            Y_hat = self.forward_propagation(X)\n",
        "            cost = self.compute_cost(Y_hat, Y)\n",
        "            self.backward_propagation(X, Y)\n",
        "            self.update_parameters(learning_rate=learning_rate)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Iteration {i} cost: {cost}\")\n",
        "\n",
        "    def fit_minibatch(self, X, Y, batch_size=32, iterations=1000, learning_rate=0.01):\n",
        "      # training using minibatch\n",
        "        m = X.shape[1]\n",
        "        for i in range(iterations):\n",
        "            # Shuffle the training data for every iteration (traditional minibatch)\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[:, permutation]\n",
        "            Y_shuffled = Y[:, permutation]\n",
        "\n",
        "            for j in range(0, m, batch_size):\n",
        "                # Get the minibatch\n",
        "                X_batch = X_shuffled[:, j:j+batch_size]\n",
        "                Y_batch = Y_shuffled[:, j:j+batch_size]\n",
        "\n",
        "                # Forward propagation\n",
        "                Y_hat = self.forward_propagation(X_batch)\n",
        "\n",
        "                # Backward propagation\n",
        "                self.backward_propagation(X_batch, Y_batch)\n",
        "\n",
        "                # Update parameters\n",
        "                self.update_parameters(learning_rate=learning_rate)\n",
        "\n",
        "            # Compute cost and print progress\n",
        "            if i % 100 == 0:\n",
        "                cost = self.compute_cost(Y_hat, Y_batch)\n",
        "                print(f\"Iteration {i}: Cost = {cost}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v35f3L_T_UyW",
        "outputId": "40725756-0f2b-443e-cec5-5fa4662c54f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Iteration 0 cost: 2.3023489444931524\n",
            "Iteration 100 cost: 2.2891630420409474\n",
            "Iteration 200 cost: 2.2645120234682157\n",
            "Iteration 300 cost: 2.212347581972457\n",
            "Iteration 400 cost: 2.1098962676700608\n",
            "Iteration 500 cost: 1.9368681002186778\n",
            "Iteration 600 cost: 1.6979682002798884\n",
            "Iteration 700 cost: 1.4401367513931542\n",
            "Iteration 800 cost: 1.217365100570236\n",
            "Iteration 900 cost: 1.0460598638753578\n",
            "Iteration 1000 cost: 0.9181809289550497\n",
            "Iteration 1100 cost: 0.8222191278108836\n",
            "Iteration 1200 cost: 0.7490369438370504\n",
            "Iteration 1300 cost: 0.6920774849168243\n",
            "Iteration 1400 cost: 0.6467746861649456\n",
            "Iteration 1500 cost: 0.6099819547314688\n",
            "Iteration 1600 cost: 0.5795250864741937\n",
            "Iteration 1700 cost: 0.5538881769341946\n",
            "Iteration 1800 cost: 0.5319981717406853\n",
            "Iteration 1900 cost: 0.513079740132881\n",
            "Total training time of traditional gradient descent:  1595.6268944740295\n",
            "Test Accuracy: 87.56%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import time\n",
        "\n",
        "# Load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to have values between 0 and 1\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Flatten the images for the neural network\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Maybe not necessary, but no harm to implement because we are dealing with multiclass problem\n",
        "y_train_encoded = to_categorical(y_train, num_classes=10).T\n",
        "y_test_encoded = to_categorical(y_test, num_classes=10).T\n",
        "\n",
        "\n",
        "# Initialize the neural network\n",
        "input_size = X_train_flatten.shape[0]  # 784\n",
        "hidden_layer_size = 128\n",
        "output_size = 10 # 0 - 9 digits\n",
        "network = NeuralNetwork(input_size, hidden_layer_size, output_size)\n",
        "\n",
        "start = time.time()\n",
        "# Train the network using traditional gradient descent\n",
        "network.fit(X_train_flatten, y_train_encoded, iterations=2000, learning_rate=0.01)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total training time of traditional gradient descent: \", end - start)\n",
        "# Evaluate the network\n",
        "predictions = network.predict(X_test_flatten)\n",
        "true_labels = np.argmax(y_test_encoded, axis=0)\n",
        "accuracy = np.mean(predictions == true_labels) * 100\n",
        "print(f\"Test Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k9Xd0ikP_pPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6433cf08-0399-4f1b-eceb-78b9d762dd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 2.251438039179879\n",
            "Iteration 100: Cost = 0.15363373820094722\n",
            "Iteration 200: Cost = 0.06892955621804037\n",
            "Iteration 300: Cost = 0.03231656078724808\n",
            "Iteration 400: Cost = 0.10353118477941037\n",
            "Iteration 500: Cost = 0.044485810708141275\n",
            "Iteration 600: Cost = 0.051494615489537515\n",
            "Iteration 700: Cost = 0.013866393656450914\n",
            "Iteration 800: Cost = 0.007281254801145713\n",
            "Iteration 900: Cost = 0.015764711518407127\n",
            "Iteration 1000: Cost = 0.037912984834571124\n",
            "Iteration 1100: Cost = 0.021827756945600043\n",
            "Iteration 1200: Cost = 0.01859885618293126\n",
            "Iteration 1300: Cost = 0.010326438159731723\n",
            "Iteration 1400: Cost = 0.012429118559501897\n",
            "Iteration 1500: Cost = 0.008193592483281989\n",
            "Iteration 1600: Cost = 0.005276336228546629\n",
            "Iteration 1700: Cost = 0.007328327594213736\n",
            "Iteration 1800: Cost = 0.01831145766096291\n",
            "Iteration 1900: Cost = 0.010566295251178711\n",
            "Total training time of mini batch gradient:  2277.392800092697\n",
            "Test Accuracy: 98.00999999999999%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to have values between 0 and 1\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Flatten the images for the neural network\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=10).T\n",
        "y_test_encoded = to_categorical(y_test, num_classes=10).T\n",
        "\n",
        "# Load and preprocess MNIST data (assuming it's already loaded and preprocessed)\n",
        "# X_train_flatten, y_train_encoded, X_test_flatten, y_test_encoded\n",
        "\n",
        "# Initialize the neural network\n",
        "input_size = X_train_flatten.shape[0]  # 784\n",
        "hidden_layer_size = 128\n",
        "output_size = 10 # 0 - 9 digits\n",
        "network = NeuralNetwork(input_size, hidden_layer_size, output_size)\n",
        "\n",
        "start = time.time()\n",
        "# Train the network using minibatch with batch size 256\n",
        "network.fit_minibatch(X_train_flatten, y_train_encoded, batch_size=256, iterations=2000, learning_rate=0.01)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(\"Total training time of mini batch gradient: \", end - start)\n",
        "# Evaluate the network\n",
        "predictions = network.predict(X_test_flatten)\n",
        "true_labels = np.argmax(y_test_encoded, axis=0)\n",
        "accuracy = np.mean(predictions == true_labels) * 100\n",
        "print(f\"Test Accuracy: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "\n",
        "The accuracy of traditional gradient descent is 87.56% and the accuracy of mini batch gradient descent is 98.01%.\n",
        "\n",
        "Here I choose minibatch to improve the performance because minibatch can make model learns more once a time comparing to stochastic gradient descent (SGD). Remember that we have images, with is a high dimensional matrix, and 10 classes versus two classes, that means the model needs to learn more details and extract more features to make successful classification. SGD might work as well, but will diverge if given only 2000 iteraions.\n",
        "\n",
        "While with the same amount of 2000 iterations, mini batch improves the accuracy a lot, showing the efficiency of the mini batch gradient descent algorithm. However, the time is longer than the traditional one, probably depends on two reasons:\n",
        "1. Colab GPU memory is big enough to hold all the data of MNIST, where MNIST is not a very huge dataset\n",
        "2. I implement the traditional mini-batch algorithm (random indexing for every iteration), while in practice we usually do the indexing at the beginning and then keep the same indexing."
      ],
      "metadata": {
        "id": "BrP6QS5Pci2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "\n",
        "Task 1 (5 points): Assuming you are not familiar with the framework, in this part of the\n",
        "homework you will present your research describing the resources you used to learn the\n",
        "framework (must include links to all resources). Clearly explain why you needed a particular\n",
        "resource for implementing a 2-layer Neural Network (NN). (Consider how you will keep track of\n",
        "all the computations in a NN i.e., what libraries/tools do you need within this framework.)"
      ],
      "metadata": {
        "id": "pNCNBKUva59A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use pytorch as the framework to code up a 2-layer Neural Network.\n",
        "I will use this link as my starting point: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "\n",
        "This link contains several examples about tensor, forward pass, backward pass, and neural network layers, which are essential for constructing neural networks.\n",
        "\n",
        "The second link that I follow is https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "\n",
        "This link focuses more on how to construct neural networks by pytorch, instead of a general introduction by example from the first link."
      ],
      "metadata": {
        "id": "Ze1V2uPNkOd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 (35 points): Once you have figured out the resources you need for the project, you\n",
        "should design and implement your project. The project must include the following steps (it’s\n",
        "not limited to these steps):\n",
        "1. Exploratory Data Analysis (Can include data cleaning, visualization etc.)\n",
        "2. Perform a train-dev-test split.\n",
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using).\n",
        "4. Compute the final cost function.\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization. Experiment with normalized inputs i.e. comment on how\n",
        "your model performs when the inputs are normalized.\n",
        "6. Present the results using the test set.\n",
        "NOTE: In this step, once you have implemented your 2-layer network you may increase and/or\n",
        "decrease the number of layers as part of the hyperparameter tuning process."
      ],
      "metadata": {
        "id": "X608XZt5JVWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data analysis:\n",
        "\n",
        "One the following example is plotted below the cell.\n",
        "According to the website http://yann.lecun.com/exdb/mnist/, there are 60,000 examples, and a test set of 10,000 example. I normalize the data in the transform below to make sure the neural network does not diverge because of large high dimensional features."
      ],
      "metadata": {
        "id": "nNACiAaJULwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "# Get one random sample from the test set\n",
        "train_data = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "data, target = next(iter(test_loader))\n",
        "data = data[0]\n",
        "target = target[0]\n",
        "# Visualize the image\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(data.squeeze().numpy(), cmap='gray')\n",
        "plt.title(f'Label: {target.item()}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwYM6grvcbKF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "aa00885a-5dee-4531-d8f1-2b9b8774408d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 348362837.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 41713393.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 212986708.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 22980131.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I use two layers, the input layer dimension is 28*28 (MNIST image size), the hidden layer is 512, and I use ReLU activation."
      ],
      "metadata": {
        "id": "Siil55NSYzwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "class NeuralNetworkPytorch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ugDVyjT7YRvH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P5OseOGrO8MW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1d9d2b-5cbe-43f4-c48d-00e0a2e789d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = NeuralNetworkPytorch().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data.to(device)).to(device)\n",
        "        loss = criterion(outputs, targets.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEzJmRTXWF_z",
        "outputId": "793648cd-2356-49d6-efab-252ca6c2008f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Batch 0/938, Loss: 2.3405\n",
            "Epoch 1/5, Batch 100/938, Loss: 0.1918\n",
            "Epoch 1/5, Batch 200/938, Loss: 0.1724\n",
            "Epoch 1/5, Batch 300/938, Loss: 0.0896\n",
            "Epoch 1/5, Batch 400/938, Loss: 0.1028\n",
            "Epoch 1/5, Batch 500/938, Loss: 0.1586\n",
            "Epoch 1/5, Batch 600/938, Loss: 0.0771\n",
            "Epoch 1/5, Batch 700/938, Loss: 0.2215\n",
            "Epoch 1/5, Batch 800/938, Loss: 0.0527\n",
            "Epoch 1/5, Batch 900/938, Loss: 0.0889\n",
            "Epoch 2/5, Batch 0/938, Loss: 0.0666\n",
            "Epoch 2/5, Batch 100/938, Loss: 0.0582\n",
            "Epoch 2/5, Batch 200/938, Loss: 0.0605\n",
            "Epoch 2/5, Batch 300/938, Loss: 0.1344\n",
            "Epoch 2/5, Batch 400/938, Loss: 0.0366\n",
            "Epoch 2/5, Batch 500/938, Loss: 0.0799\n",
            "Epoch 2/5, Batch 600/938, Loss: 0.0573\n",
            "Epoch 2/5, Batch 700/938, Loss: 0.0556\n",
            "Epoch 2/5, Batch 800/938, Loss: 0.0418\n",
            "Epoch 2/5, Batch 900/938, Loss: 0.0834\n",
            "Epoch 3/5, Batch 0/938, Loss: 0.1050\n",
            "Epoch 3/5, Batch 100/938, Loss: 0.0188\n",
            "Epoch 3/5, Batch 200/938, Loss: 0.0855\n",
            "Epoch 3/5, Batch 300/938, Loss: 0.0879\n",
            "Epoch 3/5, Batch 400/938, Loss: 0.0697\n",
            "Epoch 3/5, Batch 500/938, Loss: 0.0267\n",
            "Epoch 3/5, Batch 600/938, Loss: 0.0255\n",
            "Epoch 3/5, Batch 700/938, Loss: 0.0561\n",
            "Epoch 3/5, Batch 800/938, Loss: 0.0586\n",
            "Epoch 3/5, Batch 900/938, Loss: 0.0541\n",
            "Epoch 4/5, Batch 0/938, Loss: 0.0338\n",
            "Epoch 4/5, Batch 100/938, Loss: 0.0130\n",
            "Epoch 4/5, Batch 200/938, Loss: 0.0920\n",
            "Epoch 4/5, Batch 300/938, Loss: 0.0668\n",
            "Epoch 4/5, Batch 400/938, Loss: 0.0138\n",
            "Epoch 4/5, Batch 500/938, Loss: 0.0049\n",
            "Epoch 4/5, Batch 600/938, Loss: 0.1443\n",
            "Epoch 4/5, Batch 700/938, Loss: 0.2996\n",
            "Epoch 4/5, Batch 800/938, Loss: 0.0339\n",
            "Epoch 4/5, Batch 900/938, Loss: 0.0236\n",
            "Epoch 5/5, Batch 0/938, Loss: 0.0318\n",
            "Epoch 5/5, Batch 100/938, Loss: 0.0387\n",
            "Epoch 5/5, Batch 200/938, Loss: 0.0427\n",
            "Epoch 5/5, Batch 300/938, Loss: 0.0069\n",
            "Epoch 5/5, Batch 400/938, Loss: 0.0062\n",
            "Epoch 5/5, Batch 500/938, Loss: 0.0108\n",
            "Epoch 5/5, Batch 600/938, Loss: 0.0243\n",
            "Epoch 5/5, Batch 700/938, Loss: 0.0199\n",
            "Epoch 5/5, Batch 800/938, Loss: 0.0045\n",
            "Epoch 5/5, Batch 900/938, Loss: 0.0041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, targets in test_loader:\n",
        "        outputs = model(data.to(device)).to(device)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets.to(device)).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy on the test set: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilvO6e5jZFgJ",
        "outputId": "88ec2605-ee10-455b-f75f-143f3aa45a01"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 97.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 (10 points): In task 2 describe how you selected the hyperparameters. What was the\n",
        "rationale behind the technique you used? Did you use regularization? Why, or why not? Did you use\n",
        "an optimization algorithm? Why or why not?"
      ],
      "metadata": {
        "id": "aoOvF4nXWWfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use 512 as the hidden size of neural network, 0.001 as the learning rate, and 64 as the batchsize to implement minibatch optimization algorithm. Besides minibatch optimization algorithm, I use the Adam optimizer mentioned a couple of lectures ago. I also normalize the images before inputting into the network, where the normalizing constants transforms.Normalize((0.1307,), (0.3081,)) coming from this link: https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset\n",
        "\n",
        "512 as hidden size because the input size is 28 * 28 = 784, which a large hidden size neural network can capture more features.\n",
        "0.001 as the learning rate works because a lot of features need to be learn by network, so a small learning rating will serve better to learn features slowly. Since the dataset is big enough, overfitting is a minor issue to consider.\n",
        "\n",
        "Adam is a good optimizer to perform. It's a popular optimizer, works well with learning rate 0.001 or even smaller learning rate, and general performs well based on different implementations of network training online.\n",
        "\n",
        "For the nomarlizing constant, it's a fix number, I follow the convention. Also, this number is computed."
      ],
      "metadata": {
        "id": "FcuyO1YAXsk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following task is for Graduate level only (6000 level):\n",
        "Task 4 (100 points): Create another baseline model (can be any model we covered so far except\n",
        "a deep learning model). Using the same training data (as above) train your model and evaluate\n",
        "results using the test set. Compare the results of both models (the Neural Network and the\n",
        "baseline model). What are the reasons for one model performing better (or not) than the\n",
        "other? Explain."
      ],
      "metadata": {
        "id": "dZN-bP6OQNwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the baseline model, I am going to use One VS All logistic regression model to classify digits, using sklearn package.\n",
        "\n",
        "Reference from: https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html"
      ],
      "metadata": {
        "id": "rV5AQpMfQpxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# # Load MNIST dataset\n",
        "# mnist = fetch_openml('mnist_784', version=1)\n",
        "# X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Load data from https://www.openml.org/d/554\n",
        "\n",
        "t0 = time.time()\n",
        "train_samples = 60000\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000\n",
        ")\n",
        "\n",
        "\n",
        "# Train one-vs-all logistic regression classifier for each digit\n",
        "classifiers = []\n",
        "for digit in range(10):\n",
        "    y_train_digit = (y_train == str(digit)).astype(int)\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train_digit)\n",
        "    classifiers.append(clf)\n",
        "\n",
        "# Predict using all classifiers\n",
        "predictions = np.vstack([clf.predict(X_test) for clf in classifiers])\n",
        "\n",
        "# Select the class with the highest probability as the predicted class\n",
        "y_pred = np.argmax(predictions, axis=0)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test.astype(int), y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7NYEvrLQoJU",
        "outputId": "29f84a4b-760b-461b-96d4-25d975dd2e84"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy by neural network is 97.73% and the accuracy of Logistic regression is 85.55%.\n",
        "\n",
        "Overhere, again, I treat the MNIST as a multiclass problem by separating one class from all the other classes, using One VS All approach. Then, I implement logistic regression based on One VS All approach.\n",
        "\n",
        "As you can see, two layer network performs much better than logistic regression becuase two layer network can capture more features than simple logistic regression."
      ],
      "metadata": {
        "id": "ax273QxCSCa4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yswjI_xrQ4gW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}